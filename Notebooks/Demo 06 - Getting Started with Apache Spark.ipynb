{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"414eab8e-9e0a-4978-ab1a-8ef8f9daa88c","known_lakehouses":[{"id":"414eab8e-9e0a-4978-ab1a-8ef8f9daa88c"}],"default_lakehouse_name":"LearningPython","default_lakehouse_workspace_id":"4f22086e-b6b4-4387-a0f6-d995822f4f63"}}},"cells":[{"cell_type":"markdown","source":["# Copy CSV Files from GitHub Repository into One Lake"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7cb18242-4c5f-44fd-a80e-afca52c50bc6"},{"cell_type":"code","source":["import csv\n","import requests\n","\n","CSV_BASE_URL = \"https://github.com/PowerBiDevCamp/Python-In-Fabric-Notebooks/raw/main/CSV/\"\n","\n","CSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n","\n","ONELAKE_FOLDER_PATH = \"Files/product_sales/\"\n","\n","# delete folder to remove any existing files in target folder\n","mssparkutils.fs.rm(ONELAKE_FOLDER_PATH, recurse=True)\n","\n","for CSV_FILE in CSV_FILES:\n","    CSV_FILE_PATH = CSV_BASE_URL + CSV_FILE\n","    with requests.get(CSV_FILE_PATH) as response:\n","        csv_content = response.content.decode('utf-8-sig')\n","        mssparkutils.fs.put(ONELAKE_FOLDER_PATH + CSV_FILE, csv_content, True)\n","        print(CSV_FILE + \" copy into One Lake\")"],"outputs":[],"execution_count":null,"metadata":{},"id":"6409b5ab-786e-4ea8-b15e-b11916cfd8c0"},{"cell_type":"markdown","source":["# Convert CSV Files to Delta Tables"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f403eac9-fc44-4547-bfd7-fec2036fe6e9"},{"cell_type":"code","source":["FOLDER_PATH = \"Files/product_sales/\"\r\n","\r\n","CSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\r\n","\r\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\r\n","\r\n","for CSV_FILE in CSV_FILES:\r\n","    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\r\n","    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\r\n","    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\r\n","    print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"c5f149ae-40fd-4c72-8424-d745b8534137","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-02T19:35:46.124892Z","session_start_time":"2023-08-02T19:35:46.3564659Z","execution_start_time":"2023-08-02T19:35:56.2455001Z","execution_finish_time":"2023-08-02T19:36:23.8196475Z","spark_jobs":{"numbers":{"SUCCEEDED":60,"FAILED":0,"UNKNOWN":0,"RUNNING":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4330,"rowCount":50,"usageDescription":"","jobId":67,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 2","submissionTime":"2023-08-02T19:36:23.250GMT","completionTime":"2023-08-02T19:36:23.278GMT","stageIds":[117,118,119],"jobGroup":"3","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":53,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4330,"dataRead":4631,"rowCount":60,"usageDescription":"","jobId":66,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 2","submissionTime":"2023-08-02T19:36:22.699GMT","completionTime":"2023-08-02T19:36:23.231GMT","stageIds":[115,116],"jobGroup":"3","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4631,"dataRead":3601,"rowCount":20,"usageDescription":"","jobId":65,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 2","submissionTime":"2023-08-02T19:36:22.448GMT","completionTime":"2023-08-02T19:36:22.515GMT","stageIds":[114],"jobGroup":"3","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1725,"rowCount":3,"usageDescription":"","jobId":64,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-08-02T19:36:21.885GMT","completionTime":"2023-08-02T19:36:22.046GMT","stageIds":[112,113],"jobGroup":"3","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":1833,"dataRead":355,"rowCount":20,"usageDescription":"","jobId":63,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-08-02T19:36:21.546GMT","completionTime":"2023-08-02T19:36:21.785GMT","stageIds":[111,110],"jobGroup":"3","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":355,"dataRead":167,"rowCount":20,"usageDescription":"","jobId":62,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-08-02T19:36:21.428GMT","completionTime":"2023-08-02T19:36:21.490GMT","stageIds":[109],"jobGroup":"3","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4325,"rowCount":50,"usageDescription":"","jobId":61,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 1","submissionTime":"2023-08-02T19:36:21.281GMT","completionTime":"2023-08-02T19:36:21.311GMT","stageIds":[107,108,106],"jobGroup":"3","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4325,"dataRead":3058,"rowCount":57,"usageDescription":"","jobId":60,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 1","submissionTime":"2023-08-02T19:36:20.743GMT","completionTime":"2023-08-02T19:36:21.263GMT","stageIds":[104,105],"jobGroup":"3","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":3058,"dataRead":2493,"rowCount":14,"usageDescription":"","jobId":59,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 1","submissionTime":"2023-08-02T19:36:20.488GMT","completionTime":"2023-08-02T19:36:20.548GMT","stageIds":[103],"jobGroup":"3","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"load at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":167,"rowCount":1,"usageDescription":"","jobId":58,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-08-02T19:36:20.169GMT","completionTime":"2023-08-02T19:36:20.214GMT","stageIds":[102],"jobGroup":"3","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4343,"rowCount":50,"usageDescription":"","jobId":57,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 3","submissionTime":"2023-08-02T19:36:19.978GMT","completionTime":"2023-08-02T19:36:20.010GMT","stageIds":[99,100,101],"jobGroup":"3","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":54,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4343,"dataRead":6259,"rowCount":63,"usageDescription":"","jobId":56,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 3","submissionTime":"2023-08-02T19:36:19.485GMT","completionTime":"2023-08-02T19:36:19.957GMT","stageIds":[97,98],"jobGroup":"3","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":6259,"dataRead":5575,"rowCount":26,"usageDescription":"","jobId":55,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 3","submissionTime":"2023-08-02T19:36:19.312GMT","completionTime":"2023-08-02T19:36:19.370GMT","stageIds":[96],"jobGroup":"3","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1815,"rowCount":3,"usageDescription":"","jobId":54,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-08-02T19:36:18.779GMT","completionTime":"2023-08-02T19:36:18.884GMT","stageIds":[94,95],"jobGroup":"3","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":3208,"dataRead":529,"rowCount":20,"usageDescription":"","jobId":53,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-08-02T19:36:18.426GMT","completionTime":"2023-08-02T19:36:18.690GMT","stageIds":[93,92],"jobGroup":"3","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":529,"dataRead":848,"rowCount":20,"usageDescription":"","jobId":52,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-08-02T19:36:18.344GMT","completionTime":"2023-08-02T19:36:18.383GMT","stageIds":[91],"jobGroup":"3","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4345,"rowCount":50,"usageDescription":"","jobId":51,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 2","submissionTime":"2023-08-02T19:36:18.223GMT","completionTime":"2023-08-02T19:36:18.253GMT","stageIds":[88,89,90],"jobGroup":"3","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":53,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4345,"dataRead":4849,"rowCount":60,"usageDescription":"","jobId":50,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 2","submissionTime":"2023-08-02T19:36:17.745GMT","completionTime":"2023-08-02T19:36:18.197GMT","stageIds":[86,87],"jobGroup":"3","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4849,"dataRead":4290,"rowCount":20,"usageDescription":"","jobId":49,"name":"toString at String.java:2994","description":"Delta: Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 2","submissionTime":"2023-08-02T19:36:17.462GMT","completionTime":"2023-08-02T19:36:17.526GMT","stageIds":[85],"jobGroup":"3","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"load at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":848,"rowCount":1,"usageDescription":"","jobId":48,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 3:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-08-02T19:36:17.133GMT","completionTime":"2023-08-02T19:36:17.183GMT","stageIds":[84],"jobGroup":"3","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"824d22af-5e93-4a38-9601-ed9220347156"},"text/plain":"StatementMeta(, c5f149ae-40fd-4c72-8424-d745b8534137, 3, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Spark dataframe saved to delta table: customers\nSpark dataframe saved to delta table: invoicedetails\nSpark dataframe saved to delta table: invoices\nSpark dataframe saved to delta table: categories\nSpark dataframe saved to delta table: products\nSpark dataframe saved to delta table: countries\n"]}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"e310a5bf-1d43-4df5-a8cd-db6d214c690d"}]}