{"cells":[{"cell_type":"markdown","source":["# Copy CSV Files from GitHub Repository into One Lake"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["import csv\n","import requests\n","\n","CSV_BASE_URL = \"https://github.com/PowerBiDevCamp/Python-In-Fabric-Notebooks/raw/main/CSV/\"\n","\n","CSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n","\n","ONELAKE_FOLDER_PATH = \"Files/product_sales/\"\n","\n","# delete folder to remove any existing files in target folder\n","mssparkutils.fs.rm(ONELAKE_FOLDER_PATH, recurse=True)\n","\n","for CSV_FILE in CSV_FILES:\n","    CSV_FILE_PATH = CSV_BASE_URL + CSV_FILE\n","    with requests.get(CSV_FILE_PATH) as response:\n","        csv_content = response.content.decode('utf-8-sig')\n","        mssparkutils.fs.put(ONELAKE_FOLDER_PATH + CSV_FILE, csv_content, True)\n","        print(CSV_FILE + \" copy into One Lake\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"aad638bf-5741-47c2-b995-390abd554f7f","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-22T17:23:04.1986464Z","session_start_time":null,"execution_start_time":"2023-06-22T17:23:04.5725133Z","execution_finish_time":"2023-06-22T17:23:09.8406202Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"UNKNOWN":0,"RUNNING":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"27bdc952-72db-4cc2-a9dd-e358a9ac31e3"},"text/plain":"StatementMeta(, aad638bf-5741-47c2-b995-390abd554f7f, 11, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Customers.csv copy into One Lake\nInvoiceDetails.csv copy into One Lake\nInvoices.csv copy into One Lake\nCategories.csv copy into One Lake\nProducts.csv copy into One Lake\nCountries.csv copy into One Lake\n"]}],"execution_count":9,"metadata":{}},{"cell_type":"markdown","source":["# Convert CSV Files into Lakehouse Tables"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["FOLDER_PATH = \"Files/product_sales/\"\r\n","\r\n","CSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\r\n","\r\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\r\n","\r\n","for CSV_FILE in CSV_FILES:\r\n","    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\r\n","    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\r\n","    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\r\n","    print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"aad638bf-5741-47c2-b995-390abd554f7f","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-22T17:24:17.1320477Z","session_start_time":null,"execution_start_time":"2023-06-22T17:24:17.4524213Z","execution_finish_time":"2023-06-22T17:24:30.3345193Z","spark_jobs":{"numbers":{"SUCCEEDED":42,"UNKNOWN":0,"RUNNING":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":4330,"rowCount":50,"jobId":99,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 1","submissionTime":"2023-06-22T17:24:29.211GMT","completionTime":"2023-06-22T17:24:29.254GMT","stageIds":[150,151,152],"jobGroup":"13","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4330,"dataRead":3058,"rowCount":57,"jobId":98,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 1","submissionTime":"2023-06-22T17:24:28.797GMT","completionTime":"2023-06-22T17:24:29.195GMT","stageIds":[148,149],"jobGroup":"13","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3058,"dataRead":2493,"rowCount":14,"jobId":97,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 1","submissionTime":"2023-06-22T17:24:28.652GMT","completionTime":"2023-06-22T17:24:28.695GMT","stageIds":[147],"jobGroup":"13","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1725,"rowCount":3,"jobId":96,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-06-22T17:24:28.084GMT","completionTime":"2023-06-22T17:24:28.247GMT","stageIds":[145,146],"jobGroup":"13","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1833,"dataRead":355,"rowCount":20,"jobId":95,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-06-22T17:24:27.826GMT","completionTime":"2023-06-22T17:24:28.012GMT","stageIds":[143,144],"jobGroup":"13","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":355,"dataRead":167,"rowCount":20,"jobId":94,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-06-22T17:24:27.760GMT","completionTime":"2023-06-22T17:24:27.792GMT","stageIds":[142],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":167,"rowCount":1,"jobId":93,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-06-22T17:24:27.615GMT","completionTime":"2023-06-22T17:24:27.648GMT","stageIds":[141],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4355,"rowCount":50,"jobId":92,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 2","submissionTime":"2023-06-22T17:24:27.452GMT","completionTime":"2023-06-22T17:24:27.477GMT","stageIds":[139,140,138],"jobGroup":"13","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":53,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4355,"dataRead":4849,"rowCount":60,"jobId":91,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 2","submissionTime":"2023-06-22T17:24:27.019GMT","completionTime":"2023-06-22T17:24:27.430GMT","stageIds":[136,137],"jobGroup":"13","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4849,"dataRead":4290,"rowCount":20,"jobId":90,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 2","submissionTime":"2023-06-22T17:24:26.836GMT","completionTime":"2023-06-22T17:24:26.895GMT","stageIds":[135],"jobGroup":"13","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":2015,"rowCount":4,"jobId":89,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-06-22T17:24:26.295GMT","completionTime":"2023-06-22T17:24:26.419GMT","stageIds":[133,134],"jobGroup":"13","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3208,"dataRead":529,"rowCount":20,"jobId":88,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-06-22T17:24:26.002GMT","completionTime":"2023-06-22T17:24:26.224GMT","stageIds":[132,131],"jobGroup":"13","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":529,"dataRead":848,"rowCount":20,"jobId":87,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-06-22T17:24:25.930GMT","completionTime":"2023-06-22T17:24:25.962GMT","stageIds":[130],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":848,"rowCount":1,"jobId":86,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-06-22T17:24:25.786GMT","completionTime":"2023-06-22T17:24:25.815GMT","stageIds":[129],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4316,"rowCount":50,"jobId":85,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 1","submissionTime":"2023-06-22T17:24:25.638GMT","completionTime":"2023-06-22T17:24:25.662GMT","stageIds":[126,127,128],"jobGroup":"13","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4316,"dataRead":3012,"rowCount":57,"jobId":84,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 1","submissionTime":"2023-06-22T17:24:25.296GMT","completionTime":"2023-06-22T17:24:25.623GMT","stageIds":[125,124],"jobGroup":"13","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3012,"dataRead":2274,"rowCount":14,"jobId":83,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 1","submissionTime":"2023-06-22T17:24:25.078GMT","completionTime":"2023-06-22T17:24:25.123GMT","stageIds":[123],"jobGroup":"13","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1680,"rowCount":3,"jobId":82,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-06-22T17:24:24.642GMT","completionTime":"2023-06-22T17:24:24.744GMT","stageIds":[121,122],"jobGroup":"13","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1227,"dataRead":135,"rowCount":6,"jobId":81,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-06-22T17:24:24.380GMT","completionTime":"2023-06-22T17:24:24.575GMT","stageIds":[119,120],"jobGroup":"13","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":135,"dataRead":50,"rowCount":6,"jobId":80,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\nFOLDER_PATH = \"Files/product_sales/\"\n\nCSV_FILES = { \"Categories.csv\", \"Countries.csv\", \"Customers.csv\", \"InvoiceDetails.csv\", \"Invoices.csv\", \"Products.csv\" }\n\nspark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n\nfor CSV_FILE in CSV_FILES:\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(FOLDER_PATH + CSV_FILE)\n    table_name =  CSV_FILE.lower().replace(\".csv\", \"\")\n    df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n    print(f\"Spark dataframe saved to delta table: {table_name}\")","submissionTime":"2023-06-22T17:24:24.316GMT","completionTime":"2023-06-22T17:24:24.347GMT","stageIds":[118],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"9e7a5fad-d72e-470c-9810-631ff3670f9e"},"text/plain":"StatementMeta(, aad638bf-5741-47c2-b995-390abd554f7f, 13, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Spark dataframe saved to delta table: customers\nSpark dataframe saved to delta table: invoicedetails\nSpark dataframe saved to delta table: invoices\nSpark dataframe saved to delta table: categories\nSpark dataframe saved to delta table: products\nSpark dataframe saved to delta table: countries\n"]}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false}}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"a4dca14e-15fe-4fea-9aa6-8ff3776fb580":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"Apples","2":"1","3":"0.75","4":"http://classresources.blob.core.windows.net/images/Apples.png","index":1},{"0":"2","1":"Bananas","2":"1","3":"0.95","4":"http://classresources.blob.core.windows.net/images/Bananas.png","index":2},{"0":"3","1":"Oranges","2":"1","3":"1.25","4":"http://classresources.blob.core.windows.net/images/Oranges.png","index":3},{"0":"4","1":"Carrots","2":"2","3":"0.95","4":"http://classresources.blob.core.windows.net/images/Carrots.png","index":4},{"0":"5","1":"Cucumbers","2":"2","3":"2.25","4":"http://classresources.blob.core.windows.net/images/Cucumbers.png","index":5},{"0":"6","1":"Potatoes","2":"2","3":"1.25","4":"http://classresources.blob.core.windows.net/images/Potatoes.png","index":6},{"0":"7","1":"Tomatoes","2":"2","3":"1.75","4":"http://classresources.blob.core.windows.net/images/Tomatoes.png","index":7},{"0":"8","1":"Milk","2":"3","3":"2.5","4":"http://classresources.blob.core.windows.net/images/Milk.png","index":8},{"0":"9","1":"Butter","2":"3","3":"2.25","4":"http://classresources.blob.core.windows.net/images/Butter.png","index":9},{"0":"10","1":"Cheese","2":"3","3":"3.75","4":"http://classresources.blob.core.windows.net/images/Cheese.png","index":10}],"schema":[{"key":"0","name":"ProductId","type":"string"},{"key":"1","name":"Product","type":"string"},{"key":"2","name":"CategoryId","type":"string"},{"key":"3","name":"ListPrice","type":"string"},{"key":"4","name":"ProductImage","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["3"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":["-1"]}},"customOptions":{}}}}},"trident":{"lakehouse":{"default_lakehouse":"414eab8e-9e0a-4978-ab1a-8ef8f9daa88c","known_lakehouses":[{"id":"414eab8e-9e0a-4978-ab1a-8ef8f9daa88c"}],"default_lakehouse_name":"LearningPython","default_lakehouse_workspace_id":"4f22086e-b6b4-4387-a0f6-d995822f4f63"}}},"nbformat":4,"nbformat_minor":0}